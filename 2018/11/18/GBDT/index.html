<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">






  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=6.4.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=6.4.1">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.4.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="提升树也是boosting家族的成员，意味着提升树也采用加法模型（基学习器线性组合）和前向分步算法。参考资料里面把Gradient Boosting，以及GBDT都解释的很透彻！ Gradient Boosting 算法Gradient Boosting:  初始化：f_0(x) = \arg\min\limits_{\rho}\sum\limits_{i=1}^NL(y_i, \rho)  fo">
<meta name="keywords" content="Loss函数,Ensemble Learning,Boosting">
<meta property="og:type" content="article">
<meta property="og:title" content="GBDT">
<meta property="og:url" content="http://yoursite.com/2018/11/18/GBDT/index.html">
<meta property="og:site_name" content="Hardcore Coder">
<meta property="og:description" content="提升树也是boosting家族的成员，意味着提升树也采用加法模型（基学习器线性组合）和前向分步算法。参考资料里面把Gradient Boosting，以及GBDT都解释的很透彻！ Gradient Boosting 算法Gradient Boosting:  初始化：f_0(x) = \arg\min\limits_{\rho}\sum\limits_{i=1}^NL(y_i, \rho)  fo">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-03-03T16:00:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GBDT">
<meta name="twitter:description" content="提升树也是boosting家族的成员，意味着提升树也采用加法模型（基学习器线性组合）和前向分步算法。参考资料里面把Gradient Boosting，以及GBDT都解释的很透彻！ Gradient Boosting 算法Gradient Boosting:  初始化：f_0(x) = \arg\min\limits_{\rho}\sum\limits_{i=1}^NL(y_i, \rho)  fo">






  <link rel="canonical" href="http://yoursite.com/2018/11/18/GBDT/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>GBDT | Hardcore Coder</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/wcfrank" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hardcore Coder</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/18/GBDT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="骚炼">
      <meta itemprop="description" content="我爱七月">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hardcore Coder">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">GBDT
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-18 00:00:00" itemprop="dateCreated datePublished" datetime="2018-11-18T00:00:00+08:00">2018-11-18</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-03-04 00:00:00" itemprop="dateModified" datetime="2020-03-04T00:00:00+08:00">2020-03-04</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>提升树也是boosting家族的成员，意味着提升树也采用加法模型（基学习器线性组合）和前向分步算法。<br>参考资料里面把Gradient Boosting，以及GBDT都解释的很透彻！</p>
<h1 id="Gradient-Boosting-算法"><a href="#Gradient-Boosting-算法" class="headerlink" title="Gradient Boosting 算法"></a>Gradient Boosting 算法</h1><p>Gradient Boosting:</p>
<ol>
<li><p>初始化：<script type="math/tex">f_0(x) = \arg\min\limits_{\rho}\sum\limits_{i=1}^NL(y_i, \rho)</script></p>
</li>
<li><p>for $m=1$ to M:<br> a. 计算负梯度：<script type="math/tex">\widetilde{y_i} = -\frac{\partial L(y_i, f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}</script>, <script type="math/tex">i=1,2,\dots,N</script><br> b. 通过最小化平方误差，用基学习器<script type="math/tex">h_m(x)</script>拟合<script type="math/tex">\widetilde{y_i}</script>，</p>
<script type="math/tex; mode=display">
 w_m = \arg\min\limits_{w}\sum\limits_{i=1}^N[\widetilde{y_i} - h_m(x_i, w)]^2</script><p> c. 使用line search确定步长$\rho_m$，使得$L$最小，</p>
<script type="math/tex; mode=display">
 \rho_m = \arg\min\limits_{\rho}\sum\limits_{i=1}^NL(y_i, f_{m-1}(x_i) + \rho h_m(x_i, w_m))</script><p> d. $f<em>m(x) = f</em>{m-1}(x) + \rho_m h_m(x, w_m)$</p>
</li>
<li><p>输出<script type="math/tex">f_M(x)</script></p>
</li>
</ol>
<h1 id="GBDT算法"><a href="#GBDT算法" class="headerlink" title="GBDT算法"></a>GBDT算法</h1><p>如果基学习器<script type="math/tex">h_m(x)</script>是<strong>回归</strong>决策树模型，则称为GBDT。</p>
<ol>
<li><p>初始化：$f<em>0(x) =\arg\min\limits</em>{\rho}\sum\limits_{i=1}^NL(y_i,\rho)$</p>
</li>
<li><p>for m=1 to M:</p>
<p> a. 计算负梯度：$\widetilde{y<em>i} = -\frac{\partial L(y_i, f</em>{m-1}(x<em>i))}{\partial f</em>{m-1}(x_i)}, ~~i=1,2,\dots,N$</p>
<p> b. 通过regression tree模型拟合：${R<em>{jm}}</em>{j=1}^J=J-terminal~node~tree({\widetilde{y<em>i}, x_i}</em>{i=1}^N)$</p>
<p> c. 通过line search确定叶子节点的权重：$\gamma<em>{jm} = \arg\min\limits</em>{\gamma}\sum\limits<em>{x\in R</em>{jm}} L(y<em>i, f</em>{m-1}(x_i)+\gamma)$</p>
<p> d. $f<em>m(x)=f</em>{m-1}(x) + \sum\limits<em>{j=1}^J\gamma</em>{jm}I(x\in R_{jm})$</p>
</li>
<li><p>输出$f_M(x)$</p>
</li>
</ol>
<p>决策树寻找最优树的过程其实依靠启发式的分裂准则，训练样本是${\widetilde{y<em>i}, x_i}$，$h_m(x_i,w) = \sum\limits</em>{j=1}^J b<em>jI(x_i\in R</em>{jm})$，<br>其中$w=(R<em>{jm},b</em>{jm})^J<em>1$，最终将N个样本划分为J个区域：${R</em>{jm}}, j=1,\dots,J$. 同时求得参数R和b。每个区域为上的样本点都是相同的输出值，即$x\in R<em>{jm}\longrightarrow h_m(x)=b</em>{jm}$. 但是b会在下一步与$\rho$结合考虑得到$\gamma<em>{jm}$，所以b步主要是得到$R</em>{jm}$.</p>
<p>b步得到了J个区域，即J个叶子节点，那么叶子节点的取值是多少？也就是这棵树到底输出多少？对于不同的损失函数，叶子节点的值也不一样。 第m颗树的第j个叶子节点的值为<script type="math/tex">\gamma_{jm}=b_{jm}\rho_m</script>。在GBDT里，通常将c步称作Shrinkage。</p>
<p>与Gradient Boosting形式一致的话 ，d步可写成$f<em>m(x)=f</em>{m-1}(x) + \rho<em>m\sum\limits</em>{j=1}^Jb<em>{jm}I(x\in R</em>{jm})$.</p>
<p> <strong>$f_m(x)$是前m轮的累积求和。</strong></p>
<h1 id="残差，负梯度，损失函数"><a href="#残差，负梯度，损失函数" class="headerlink" title="残差，负梯度，损失函数"></a>残差，负梯度，损失函数</h1><p>一方面，对于加法模型，经过迭代，在每一步$m$得到$f_m(x)$，与真实值的差即为残差：$r=y-f_m(x)$.</p>
<p>另一方面，GBDT每一步$m$迭代拟合的是负梯度，是损失函数关于上一轮预测值的负梯度$\frac{\partial L(y<em>i, f</em>{m-1}(x<em>i))}{\partial f</em>{m-1}(x_i)}$，预测值在负梯度方向上使得损失函数减小。</p>
<p><strong>问题：</strong>每一步迭代的目的，是缩小残差吗？如果是的话，拟合出来的值$\gamma<em>{jm}I(x\in R</em>{jm})$与之前的<script type="math/tex">f_{m-1}</script>相加<script type="math/tex">f_m=f_{m-1}+ Fitted Residual</script>，最终拟合预测值$f_M$.</p>
<p>回想泰勒展开，函数的变化量可以展开为各阶导数之和。也就是残差可以表示为各阶导数的线性组合，我们这里只考虑一阶导数，只用一阶导数（负梯度）这一项来近似表示残差。</p>
<h2 id="1-square-loss"><a href="#1-square-loss" class="headerlink" title="1.square loss"></a>1.square loss</h2><p><strong>回归问题</strong>常用的损失函数为square loss $l(y_i,\hat{y}_i)=(y_i-\hat{y}_i)^2$可以很好的解释gradient boosting，拟合的负梯度就是残差，因为拟合的损失函数与泰勒展开近似之后的形式完全一样：</p>
<script type="math/tex; mode=display">\begin{array}{ll}
Loss^{(t)} & = \sum\limits_{i=1}^n [y_i - (\hat{y}_i^{(t-1)}+f_t(x_i))]^2 + \Omega(f_t) + const \\
          & = \sum\limits_{i=1}^n [(y_i-\hat{y}_i^{(t-1)})^2 + 2(\hat{y}_i^{(t-1)}-y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + const 
\end{array}</script><p>回忆泰勒展开式$f(x+\Delta x) \simeq f(x) + f’(x)\Delta x + \frac{1}{2}f’’(x)\Delta x^2$，所以在迭代后mean square loss损失函数的展开形式与泰勒展开形式完全一样。</p>
<h2 id="2-logloss"><a href="#2-logloss" class="headerlink" title="2. logloss"></a>2. logloss</h2><p><strong>二分类问题</strong>常用对数损失函数。</p>
<p>回顾从线性回归到逻辑回归：是将线性拟合的值，经过logistics函数$h<em>{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}$，$h</em>{\theta}(x)$的值表示结果是1的概率。所以$P(y=1|x,\theta)=h<em>{\theta}(x), P(y=0|x,\theta)=1-h</em>{\theta}(x)$. 综合起来可写成$P(y|x,\theta)=(h<em>{\theta}(x))^y(1-h</em>{\theta}(x))^{1-y}$. 对数似然为$\prod\limits<em>{i=1}^N(h</em>{\theta}(x<em>i))^{y_i}(1-h</em>{\theta}(x_i))^{1-y_i}$，连乘不方便计算，取对数变成求和，即为对数损失函数。因为似然函数是求最大值时的$\theta$，所以这里取负号求最小化损失函数。</p>
<script type="math/tex; mode=display">
L(\theta)=-\frac{1}{N}\sum\limits_{i=1}^N[y_i\log h_{\theta}(x_i)+(1-y_i)\log(1-h_{\theta}(x_i))]</script><p>GBDT用的是回归树来解决分类问题，与逻辑回归的思想有点类似，同样是先线性拟合，然后再求概率。二分类问题的GBDT也常用对数损失函数。</p>
<p>对单个样本$i$，损失函数为$-y<em>i\log\hat{y}_i-(1-y_i)\log(1-\hat{y}_i)$，其中$\hat{y}_i=h</em>{\theta}(x)=\frac{1}{1+e^{-f_m(x_i)}}$，其中$f_m(x_i)$是m轮迭代后树模型相加的值。将$\hat{y}_i$替换为$f_m(x)$后，损失函数可写为</p>
<script type="math/tex; mode=display">
L(y_i,f_m(x_i))=y_i\log(1+e^{-f_m(x_i)})+(1-y_i)[f_m(x_i)+\log(1+e^{-f_m(x_i)})]</script><p>损失函数的负梯度为</p>
<script type="math/tex; mode=display">
r_{im}=-\bigg|\frac{\partial L(y_i, f_m(x_i))}{\partial f_m(x_i)}\bigg|= y_i-\frac{1}{1+e^{-f_m(x_i)}}=y_i-\hat{y}_i</script><p>可以看到，从形式上跟回归问题的负梯度的一致的！</p>
<p>但是对数损失函数在求line serach的时候，$\gamma<em>{jm} = \arg\min\limits</em>{\gamma}\sum\limits<em>{x\in R</em>{jm}} L(y<em>i, f</em>{m-1}(x_i)+\gamma)$没有闭式解，所以用近似值代替（推导见[6-2]）：</p>
<script type="math/tex; mode=display">
\gamma_{jm} = \frac{\sum\limits_{x_i\in R_{jm}}r_{im}}{\sum\limits_{x_i\in R_{jm}}(y_i-r_{im})(1-y_i+r_{im})}</script><p>就是用负梯度以及$y<em>i$来近似代替line search得到的shrinkage $\gamma</em>{jm}$. 原本负梯度只是用来当作基模型的label，这里还多了一个用途。</p>
<p>另外，初始化的基模型为$f_0(x) = \log\frac{p}{1-p}$，这是因为类似于逻辑回归中的$\theta^Tx=\log\frac{p}{1-p}$，GBDT中的$f(x)$就类似于逻辑回归中的$\theta^Tx$.  二元GBDT分类算法和逻辑回归思想一样，用一系列的梯度提升树去拟合这个对数几率$\log\frac{p}{1-p}$，二分类模型的最终表达式为</p>
<script type="math/tex; mode=display">
P(y=1|x)=\frac{1}{1+e^{-F_M(x)}}</script><h2 id="3-cross-entropy"><a href="#3-cross-entropy" class="headerlink" title="3. cross-entropy"></a>3. cross-entropy</h2><p><strong>多分类问题</strong>，相较于逻辑回归，假设有K类标签，样本结果为k的概率是</p>
<script type="math/tex; mode=display">
P(y=t|x,\theta) = \frac{e^{\theta_t^Tx}}{\sum\limits_{k=1}^Ke^{\theta_k^Tx}}</script><p>即通过softmax将预测结果归一化，如果不是线性模型，用$h_{\theta}(x)$来表示$\theta^Tx$。softmax一般也对应着cross-entropy损失函数（单个样本）</p>
<script type="math/tex; mode=display">
Loss =  -\log \prod\limits_{k=1}^KP(y_k|x)^{y_k}=-\sum\limits_{k=1}^Ky_k\log(h_{\theta_k}(x))</script><p>GBDT在处理多分类问题的时候，实际上<strong>每一轮都训练K颗树</strong>，每一类都训练一个0-1二分类的回归树模型（属于第k类的样本标签为1，不属于第k类的标签为0），去拟合softmax的K个分支。</p>
<p>用$f(x)$代替$h_{\theta}(x)$，则<strong>单个样本</strong>的损失函数可写为</p>
<script type="math/tex; mode=display">
Loss(y, f_{1}(x),f_2(x), \dots, f_K(x)) = -\sum\limits_{k=1}^Ky_k\log(h_{\theta_k}(x)) = -\sum\limits_{k=1}^Ky_k\log\frac{e^{f_{k}(x)}}{\sum\limits_{k'=1}^Ke^{f_{k'}(x)}}</script><p>这里是单个样本的损失函数，注意$f_k(x)$并非指第k轮迭代，而是在同一轮迭代中第k类分支的树模型。该样本在第k类的负梯度为</p>
<script type="math/tex; mode=display">
-\frac{\partial loss}{\partial f_k(x)}=y_k-\frac{e^{f_k(x)}}{\sum_{k'=1}^Ke^{f_{k'}(x)}} = y_k-\hat{y}_k</script><p>这个形式同样与square loss和logloss保持一致，同样认为是在拟合样本真实值与预测值的概率差。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p>前面介绍了三种常见的损失函数square loss，logloss和cross-entropy，分别用于回归问题、二分类和多分类问题。<strong>虽然损失函数的形式不同，GBDT拟合的负梯度，都可以理解成在拟合残差</strong>。只不过square loss就是真实值与预测值之差。而logloss和cross-entropy的残差是指真实值与预测概率之差，每一轮得到的$f(x)$需要经过logistic函数或softmax来变成概率，拟合上一轮概率值的残差：$y<em>i-\frac{1}{1+e^{-F</em>{m-1}(x_i)}}$(这里以二分类为例)。分类的残差拟合不容易理解，就是每一轮训练的回归树，是需要经过变换成为概率值来拟合的。</p>
<h1 id="用Decision-Tree来拟合残差"><a href="#用Decision-Tree来拟合残差" class="headerlink" title="用Decision Tree来拟合残差"></a>用Decision Tree来拟合残差</h1><ul>
<li>无论是回归问题还是分类问题，都是用回归决策树来拟合</li>
<li>决策树可以把样本划分为多个区域，每个区域为上的样本点都是相同的输出值：<br>  单颗决策树可表示为<script type="math/tex">h(x,(R_j,b_j)^J_1)=\sum\limits_{j=1}^J b_jI(x\in R_j)</script><br>  其中，<script type="math/tex">R_j</script>为<script type="math/tex">J</script>个独立区域（即各个叶子结点），<script type="math/tex">b_j</script>为各区域上的输出值<br>  于是，2.b可写成<script type="math/tex">(R_{jm})_1^J=\arg\min\limits_{R_{jm}}\sum\limits_{i=1}^N[\widetilde{y_i} - h_m(x_i,(R_{jm},b_{jm})^J_1)]^2</script><!-- - 回归问题的值为该区域样本点的均值，分类问题是众数 --></li>
<li>无论是回归问题还是分类问题，都是用回归树来拟合。<br>  即使是分类问题，也是使用回归树来拟合，比如使用logloss来拟合，拟合的是概率值。关于logloss的讨论，参考下面两篇<br>  <a href="https://github.com/wcfrank/GBDT/blob/master/2.%20Loss%20functions.ipynb" target="_blank" rel="noopener">https://github.com/wcfrank/GBDT/blob/master/2.%20Loss%20functions.ipynb</a><br>  <a href="https://github.com/wcfrank/GBDT/blob/master/3.%20compute_loss.ipynb" target="_blank" rel="noopener">https://github.com/wcfrank/GBDT/blob/master/3.%20compute_loss.ipynb</a></li>
<li>2.b得到<script type="math/tex">R_{jm}</script>，2.c得到<script type="math/tex">\gamma_{jm}</script>：用回归树来拟合出<script type="math/tex">R_{jm}</script>，令<script type="math/tex">\gamma_{jm}=\rho_mb_{jm}</script>，2.b和2.c可以结合写成<script type="math/tex; mode=display">\gamma_{jm} = \arg\min\limits_{\gamma}\sum\limits_{x_i\in R_{jm}}L(y_i, f_{m-1}(x_i)+\gamma)</script><ul>
<li>如果是Squared loss，最优值为region中残差的平均数，<script type="math/tex">\gamma_{jm} = \arg\min\limits_{\gamma}\sum\limits_{x_i\in R_{jm}}((y_i-f_{m-1}(x_i))-\gamma)^2 = average_{x_i\in R_{jm}}\widetilde{y_i}</script></li>
<li>如果是Absolute loss，最优值为region中残差的中位数，<script type="math/tex">\gamma_{jm} = \arg\min\limits_{\gamma}\sum\limits_{x_i\in R_{jm}}|(y_i-f_{m-1}(x_i))-\gamma| = median_{x_i\in R_{jm}} (y_i - f_{m-1}(x_i))</script></li>
<li>如果是Log loss，<script type="math/tex">\gamma_{jm} = \arg\min\limits_{\gamma}\sum\limits_{x_i\in R_{jm}}\log(1+e^{-2y_i(f_{m-1}(x_i)+\gamma)})\approx \frac{\sum\limits_{x_i\in R_{jm}}\widetilde{\widetilde{y_i}}}{\sum\limits_{x_i\in R_{jm}}|\widetilde{y_i}|(2-\widetilde{y_i})}</script>，见参考资料5</li>
</ul>
</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol>
<li><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener">Introduction to Boosted Trees (Chen Tianqi)</a></li>
<li><a href="https://www.cnblogs.com/ScorpioLu/p/8296994.html" target="_blank" rel="noopener">GBDT原理详解</a></li>
<li><a href="https://www.youtube.com/watch?v=ErDgauqnTHk" target="_blank" rel="noopener">Introduction To Gradient Boosting algorithm (simplistic n graphical) - Machine Learning</a></li>
<li><a href="https://explained.ai/gradient-boosting/index.html" target="_blank" rel="noopener">How to explain gradient boosting</a></li>
<li><a href="https://blog.csdn.net/qq_22238533/article/details/79185969" target="_blank" rel="noopener">GBDT原理与Sklearn源码分析-回归篇</a></li>
<li><p>Mictrostrong专栏</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/81016622" target="_blank" rel="noopener">深入理解GBDT回归算法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/89549390" target="_blank" rel="noopener">深入理解GBDT二分类算法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/91652813" target="_blank" rel="noopener">深入理解GBDT多分类算法</a></li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/46445201" target="_blank" rel="noopener">GBDT算法用于分类问题</a></p>
</li>
</ol>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Loss函数/" rel="tag"># Loss函数</a>
          
            <a href="/tags/Ensemble-Learning/" rel="tag"># Ensemble Learning</a>
          
            <a href="/tags/Boosting/" rel="tag"># Boosting</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/04/adaboost/" rel="next" title="Adaboost">
                <i class="fa fa-chevron-left"></i> Adaboost
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/06/decision_tree/" rel="prev" title="决策树">
                决策树 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">骚炼</p>
              <p class="site-description motion-element" itemprop="description">我爱七月</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/wcfrank" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:wangchao0519@hotmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Gradient-Boosting-算法"><span class="nav-number">1.</span> <span class="nav-text">Gradient Boosting 算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GBDT算法"><span class="nav-number">2.</span> <span class="nav-text">GBDT算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#残差，负梯度，损失函数"><span class="nav-number">3.</span> <span class="nav-text">残差，负梯度，损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-square-loss"><span class="nav-number">3.1.</span> <span class="nav-text">1.square loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-logloss"><span class="nav-number">3.2.</span> <span class="nav-text">2. logloss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-cross-entropy"><span class="nav-number">3.3.</span> <span class="nav-text">3. cross-entropy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-总结"><span class="nav-number">3.4.</span> <span class="nav-text">4. 总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#用Decision-Tree来拟合残差"><span class="nav-number">4.</span> <span class="nav-text">用Decision Tree来拟合残差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">骚炼</span>

  

  
</div>





  <div class="powered-by">
    <i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
      本站访客数:<span id="busuanzi_value_site_uv"></span>
    </span>
  </div>

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Mist</a> v6.4.1</div>




<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共46.2k字</span>
</div>
        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script>



  



  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
  


  
  

  

  

  

  

  

</body>
</html>
